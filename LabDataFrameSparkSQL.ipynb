{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Installing required packages\n",
        "!pip install pyspark\n",
        "!pip install findspark"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /Users/lisboa/opt/anaconda3/lib/python3.9/site-packages (3.3.1)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /Users/lisboa/opt/anaconda3/lib/python3.9/site-packages (from pyspark) (0.10.9.5)\n",
            "Requirement already satisfied: findspark in /Users/lisboa/opt/anaconda3/lib/python3.9/site-packages (2.0.1)\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark is the Spark API for Python. In this lab, we use PySpark to initialize the spark context. \n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1 -  Spark Context and Spark Session\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise, you will create the Spark Context and initialize the Spark session needed for SparkSQL and DataFrames.\n",
        "SparkContext is the entry point for Spark applications and contains functions to create RDDs such as `parallelize()`. SparkSession is needed for SparkSQL and DataFrame operations.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 1: Creating the spark session and context\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a spark context class\n",
        "sc = SparkContext()\n",
        "\n",
        "# Creating a spark session\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark DataFrames example\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/12/16 12:34:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "22/12/16 12:34:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3: DataFrames and SparkSQL\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to work with the extremely powerful SQL engine in Apache Spark, you will need a Spark Session. We have created that in the first Exercise, let us verify that spark session is still active.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 1: Create a DataFrame!\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can create a structured data set (much like a database table) in Spark.  Once you have done that, you can then use powerful SQL tools to query and join your dataframes.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the data first into a local `people.json` file\n",
        "\n",
        "!curl https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/people.json >> people.json\n",
        "\n",
        "#!curl https://raw.githubusercontent.com/PacktPublishing/Jupyter-for-Data-Science/master/Chapter04/files/people.json >> people.json"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100    73  100    73    0     0     95      0 --:--:-- --:--:-- --:--:--    94\n"
          ]
        }
      ],
      "execution_count": 22,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the dataset into a spark dataframe using the `read.json()` function\n",
        "df = spark.read.json(\"people.json\").cache()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/12/16 12:44:31 WARN CacheManager: Asked to cache already cached data.\n"
          ]
        }
      ],
      "execution_count": 23,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the dataframe as well as the data schema\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+--------------------+---------------+\n",
            "| age|born|                fame|           name|\n",
            "+----+----+--------------------+---------------+\n",
            "|null|null|                null|        Michael|\n",
            "|  30|null|                null|           Andy|\n",
            "|  19|null|                null|         Justin|\n",
            "|null|null|                null|        Michael|\n",
            "|  30|null|                null|           Andy|\n",
            "|  19|null|                null|         Justin|\n",
            "|null|null|                null|        Michael|\n",
            "|  30|null|                null|           Andy|\n",
            "|  19|null|                null|         Justin|\n",
            "|null|1955|co-founder of App...|     Steve Jobs|\n",
            "|null|1955|                null|Tim Berners-Lee|\n",
            "|null|1815|                null|   George Boole|\n",
            "+----+----+--------------------+---------------+\n",
            "\n",
            "root\n",
            " |-- age: long (nullable = true)\n",
            " |-- born: long (nullable = true)\n",
            " |-- fame: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "execution_count": 24,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the DataFrame as a SQL temporary view\n",
        "df.createTempView(\"people\")"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "Temporary view 'people' already exists",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Register the DataFrame as a SQL temporary view\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateTempView\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpeople\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py:256\u001b[0m, in \u001b[0;36mDataFrame.createTempView\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreateTempView\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;124;03m\"\"\"Creates a local temporary view with this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \n\u001b[1;32m    235\u001b[0m \u001b[38;5;124;03m    The lifetime of this temporary table is tied to the :class:`SparkSession`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m \n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateTempView\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Temporary view 'people' already exists"
          ]
        }
      ],
      "execution_count": 25,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 2: Explore the data using DataFrame functions and SparkSQL\n",
        "\n",
        "In this section, we explore the datasets using functions both from dataframes as well as corresponding SQL queries using sparksql. Note the different ways to achieve the same task!\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Select and show basic data columns\n",
        "\n",
        "df.select(\"name\").show()\n",
        "df.select(df[\"name\"]).show()\n",
        "spark.sql(\"SELECT name FROM people\").show()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+\n",
            "|           name|\n",
            "+---------------+\n",
            "|        Michael|\n",
            "|           Andy|\n",
            "|         Justin|\n",
            "|        Michael|\n",
            "|           Andy|\n",
            "|         Justin|\n",
            "|        Michael|\n",
            "|           Andy|\n",
            "|         Justin|\n",
            "|     Steve Jobs|\n",
            "|Tim Berners-Lee|\n",
            "|   George Boole|\n",
            "+---------------+\n",
            "\n",
            "+---------------+\n",
            "|           name|\n",
            "+---------------+\n",
            "|        Michael|\n",
            "|           Andy|\n",
            "|         Justin|\n",
            "|        Michael|\n",
            "|           Andy|\n",
            "|         Justin|\n",
            "|        Michael|\n",
            "|           Andy|\n",
            "|         Justin|\n",
            "|     Steve Jobs|\n",
            "|Tim Berners-Lee|\n",
            "|   George Boole|\n",
            "+---------------+\n",
            "\n",
            "+---------------+\n",
            "|           name|\n",
            "+---------------+\n",
            "|        Michael|\n",
            "|           Andy|\n",
            "|         Justin|\n",
            "|        Michael|\n",
            "|           Andy|\n",
            "|         Justin|\n",
            "|        Michael|\n",
            "|           Andy|\n",
            "|         Justin|\n",
            "|     Steve Jobs|\n",
            "|Tim Berners-Lee|\n",
            "|   George Boole|\n",
            "+---------------+\n",
            "\n"
          ]
        }
      ],
      "execution_count": 26,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform basic filtering\n",
        "\n",
        "df.filter(df[\"age\"] > 21).show()\n",
        "spark.sql(\"SELECT age, name FROM people WHERE age > 21\").show()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+----+----+\n",
            "|age|born|fame|name|\n",
            "+---+----+----+----+\n",
            "| 30|null|null|Andy|\n",
            "| 30|null|null|Andy|\n",
            "| 30|null|null|Andy|\n",
            "+---+----+----+----+\n",
            "\n",
            "+---+----+\n",
            "|age|name|\n",
            "+---+----+\n",
            "| 30|Andy|\n",
            "| 30|Andy|\n",
            "| 30|Andy|\n",
            "+---+----+\n",
            "\n"
          ]
        }
      ],
      "execution_count": 27,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Perfom basic aggregation of data\n",
        "\n",
        "df.groupBy(\"age\").count().show()\n",
        "spark.sql(\"SELECT age, COUNT(age) as count FROM people GROUP BY age\").show()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+\n",
            "| age|count|\n",
            "+----+-----+\n",
            "|  19|    3|\n",
            "|null|    6|\n",
            "|  30|    3|\n",
            "+----+-----+\n",
            "\n",
            "+----+-----+\n",
            "| age|count|\n",
            "+----+-----+\n",
            "|  19|    3|\n",
            "|null|    0|\n",
            "|  30|    3|\n",
            "+----+-----+\n",
            "\n"
          ]
        }
      ],
      "execution_count": 28,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1 - RDDs\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create an RDD with integers from 1-50. Apply a transformation to multiply every number by 2, resulting in an RDD that contains the first 50 even numbers.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# starter code\n",
        "# numbers = range(1, 50)\n",
        "# numbers_RDD = ...\n",
        "# even_numbers_RDD = numbers_RDD.map(lambda x: ..)"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Code block for learners to answer"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2 - DataFrames and SparkSQL\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to the `people.json` file, now read the `people2.json` file into the notebook, load it into a dataframe and apply SQL operations to determine the average age in our people2 file.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# starter code\n",
        "# !curl https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/people2.json >> people2.json\n",
        "# df = spark.read...\n",
        "# df.createTempView..\n",
        "# spark.sql(\"SELECT ...\")"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Code block for learners to answer"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Double-click **here** for a hint.\n",
        "\n",
        "<!-- The hint is below:\n",
        "\n",
        "1. The SQL query \"Select AVG(column_name) from..\" can be used to find the average value of a column. \n",
        "2. Another possible way is to use the dataframe operations select() and mean()\n",
        "-->\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Double-click **here** for the solution.\n",
        "\n",
        "<!-- The answer is below:\n",
        "df = spark.read('people2.json')\n",
        "df.createTempView(\"people2\")\n",
        "spark.sql(\"SELECT AVG(age) from people2\")\n",
        "-->\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3 - SparkSession\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Close the SparkSession we created for this notebook\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Code block for learners to answer"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Double-click **here** for the solution.\n",
        "\n",
        "<!-- The answer is below:\n",
        "\n",
        "spark.stop() will stop the spark session\n",
        "\n",
        "-->\n"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "nteract": {
      "version": "0.28.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}