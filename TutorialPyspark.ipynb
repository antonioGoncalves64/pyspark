{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPacz/Ni+LLMJlvZx4EEtEj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antonioGoncalves64/pyspark/blob/main/TutorialPyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data from repository\n",
        "\n",
        "Get external date that will be used in this  tutorial.\n"
      ],
      "metadata": {
        "id": "veh6BUkIKQAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties-2021.csv\n",
        "!wget https://perso.telecom-paristech.fr/eagan/class/igr204/data/cars.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYQuiovoKhyS",
        "outputId": "680fe437-ae20-4bc1-c904-fd033350dbff"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-16 09:33:21--  https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties-2021.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 50311433 (48M) [text/plain]\n",
            "Saving to: ‘us-counties-2021.csv’\n",
            "\n",
            "us-counties-2021.cs 100%[===================>]  47.98M   190MB/s    in 0.3s    \n",
            "\n",
            "2022-12-16 09:33:23 (190 MB/s) - ‘us-counties-2021.csv’ saved [50311433/50311433]\n",
            "\n",
            "--2022-12-16 09:33:23--  https://perso.telecom-paristech.fr/eagan/class/igr204/data/cars.csv\n",
            "Resolving perso.telecom-paristech.fr (perso.telecom-paristech.fr)... 137.194.22.227, 2a04:8ec0:0:a::89c2:16e3\n",
            "Connecting to perso.telecom-paristech.fr (perso.telecom-paristech.fr)|137.194.22.227|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22663 (22K) [text/csv]\n",
            "Saving to: ‘cars.csv’\n",
            "\n",
            "cars.csv            100%[===================>]  22.13K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-12-16 09:33:24 (153 MB/s) - ‘cars.csv’ saved [22663/22663]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automatic Installation pySpark\n",
        "\n",
        "The easy way of installing PySpark on Google Colab is to use pip install."
      ],
      "metadata": {
        "id": "y5TC_wFBcbhG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uu32wx-2bIK7",
        "outputId": "3d7cc69f-5b5c-4833-98e7-f1e4bd9ad6f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 38 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 78.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=b6be8a31d23fd5261f66e881090bf753779f38628289c7a161e5d8197f11b770\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ],
      "source": [
        "! pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a Spark Session\n",
        "After installation, we can create a Spark context and check its information."
      ],
      "metadata": {
        "id": "2TjBgdBYc4Z1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf \n",
        "from pyspark.context import SparkContext \n",
        "\n",
        "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\").setAppName(\"Intro pyspark\"))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "sc\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "XIauV_VmdaOJ",
        "outputId": "8f858a3b-9478-4aea-bdbe-08dbb5ac6d65"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local[*] appName=Intro pyspark>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://9a175d10a490:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Intro pyspark</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resilient Distributed Datasets (RDD)\n",
        "Spark revolves around the concept of a resilient distributed dataset (RDD), which is a fault-tolerant collection of elements that can be operated on in parallel."
      ],
      "metadata": {
        "id": "S_x0ahKgedgz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to create RDDs\n",
        "\n",
        "There are two ways to create RDDs:\n",
        "\n",
        "* parallelizing an existing collection in your driver program, or\n",
        "* referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat."
      ],
      "metadata": {
        "id": "NqxCeQ7DeyEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parallelized collection\n",
        "\n",
        "Parallelized collections are created by calling SparkContext’s parallelize method on an existing iterable or collection in your driver program. The elements of the collection are copied to form a distributed dataset that can be operated on in parallel. For example, here is how to create two parallelized collection: one for holding the numbers 1 to 5 and the other for hoding a String."
      ],
      "metadata": {
        "id": "1WOLHqqDfrtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numRDD = sc.parallelize([1,2,3,4,5])\n",
        "print (\"numRDD:   \", type(numRDD)) #confirm type of object RDD\n",
        "\n",
        "helloRDD = sc.parallelize((\"Hello world\"))\n",
        "print (\"helloRDD: \",type(helloRDD)) #confirm type of object RDD"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "E5ztdsxefEnN",
        "outputId": "11c904be-ec27-49a3-ea75-f7119a9d4346"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-23f50a3bd8cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnumRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"numRDD:   \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#confirm type of object RDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhelloRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hello world\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"helloRDD: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhelloRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#confirm type of object RDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once created, the distributed dataset (distData) can be operated on in parallel. For example, we can call distData.reduce(lambda a, b: a + b) to add up the elements of the list."
      ],
      "metadata": {
        "id": "FGwFtO39gCpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distData = sc.parallelize([1,2,3])\n",
        "distData.reduce(lambda a, b: a + b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "aTPrNBTwgGnC",
        "outputId": "4d801b1f-e634-4203-f73a-596fbf4707f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-33563bfe75d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdistData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdistData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One important parameter for parallel collections is the number of partitions to cut the dataset into. Spark will run one task for each partition of the cluster. Typically you want 2-4 partitions for each CPU in your cluster. Normally, Spark tries to set the number of partitions automatically based on your cluster. However, you can also set it manually by passing it as a second parameter to parallelize (e.g. sc.parallelize(data, 10))"
      ],
      "metadata": {
        "id": "RxvHIDiNgMAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data01 = sc.parallelize([1,2,3,4,5])\n",
        "print (\"Data01 NumPartitions: \", data01.getNumPartitions())\n",
        "\n",
        "data02 = sc.parallelize([1,2,3,4,5],3)\n",
        "print (\"data02 NumPartitions: \", data02.getNumPartitions())"
      ],
      "metadata": {
        "id": "TbbmSxvOgNuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### External Datasets\n",
        "PySpark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat.\n",
        "\n",
        "Text file RDDs can be created using SparkContext’s textFile method. This method takes a URI for the file (either a local path on the machine, or a hdfs://, s3a://, etc URI) and reads it as a collection of lines. Here is an example invocation:"
      ],
      "metadata": {
        "id": "CoFolhdgiTPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://perso.telecom-paristech.fr/eagan/class/igr204/data/cars.csv\n",
        "\n",
        "fileRDD = sc.textFile(\"cars.csv\")\n",
        "\n",
        "newRDD= fileRDD.take(3)\n",
        "\n",
        "for i in newRDD:\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "i3CvUU_siqi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RDD Operations\n",
        "\n",
        "RDDs support two types of operations: transformations and actions\n",
        " \n",
        "* Transformations  create a new dataset from an existing one.\n",
        "* Actions, which return a value to the driver program after running a computation on the dataset. \n",
        "\n",
        "For example, map is a transformation that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, reduce is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program (although there is also a parallel reduceByKey that returns a distributed dataset).\n",
        "\n",
        "All transformations in Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. This design enables Spark to run more efficiently. For example, we can realize that a dataset created through map will be used in a reduce and return only the result of the reduce to the driver, rather than the larger mapped dataset.\n"
      ],
      "metadata": {
        "id": "FVKUL-IVepVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Transformations\n"
      ],
      "metadata": {
        "id": "zg2yz8pnhYLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# map( ) - Return a new RDD by applying a function to each element of this RDD.\n",
        "\n",
        "RDD = sc.parallelize([1,2,3,4,5])\n",
        "RDD_map = RDD.map(lambda x : x * 2)\n",
        "print (\"RDD_map: \",RDD_map.collect()) # action convert to a  List"
      ],
      "metadata": {
        "id": "sa6vX4Ilm3vS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter( ) returns a new RDD with only the elements that pass the condition\n",
        "\n",
        "RDD = sc.parallelize([1,2,3,4])\n",
        "RDD_filter = RDD.filter(lambda x : x >2)\n",
        "print (\"RDD_filter: \", RDD_filter.collect()) # action convert to a  List"
      ],
      "metadata": {
        "id": "ICjGylIDnG3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# flatMap( ) returns multiple values for each element in the original RDD\n",
        "\n",
        "RDD = sc.parallelize([\"hello word\", \"How are you\"])\n",
        "RDD_flatMap = RDD.flatMap(lambda x : x.split(\" \"))\n",
        "print (\"RDD_flatMap: \", RDD_flatMap.collect()) # action convert to a  List"
      ],
      "metadata": {
        "id": "YczEBvB7nHG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# union( ) Return the union of this RDD and another one\n",
        "\n",
        "rdd01 = sc.parallelize([1, 3, 5, 7])\n",
        "rdd02 = sc.parallelize([2, 4, 6, 8])\n",
        "rdd03 = rdd01.union(rdd02)\n",
        "rdd03.collect()"
      ],
      "metadata": {
        "id": "jho6OzISnHQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Actions"
      ],
      "metadata": {
        "id": "eNcDqouUoDXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# collection ( ) Return a list that contains all of the elements in this RDD\n",
        "\n",
        "data = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
        "rdd  = sc.parallelize(data)\n",
        "\n",
        "newData = rdd.collect()\n",
        "for d in newData:\n",
        "    print (f\"Value: {d}\")"
      ],
      "metadata": {
        "id": "stkC1bThoPNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take(num) – Take the first num elements of the RDD\n",
        "\n",
        "data = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
        "rdd  = sc.parallelize(data)\n",
        "\n",
        "newData = rdd.take(2)\n",
        "for d in newData:\n",
        "    print (f\"Value: {d}\")"
      ],
      "metadata": {
        "id": "GwEaAKFioYVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first( ) – Returns the first record of the RDD\n",
        "\n",
        "data = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
        "rdd  = sc.parallelize(data)\n",
        "\n",
        "newData = rdd.first()\n",
        "print (f\"Value: {newData}\")"
      ],
      "metadata": {
        "id": "-_Z4xlYvogo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count( ) – Returns the number of records in an RDD\n",
        "\n",
        "data = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
        "rdd  = sc.parallelize(data)\n",
        "\n",
        "num = rdd.count()\n",
        "print (f\"Count: {num}\")"
      ],
      "metadata": {
        "id": "4bqBHK_jomPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# max( ) – Returns max record\n",
        "\n",
        "data = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
        "rdd  = sc.parallelize(data)\n",
        "\n",
        "num = rdd.max()\n",
        "print (f\"Max: {num}\")"
      ],
      "metadata": {
        "id": "xuuLupgAomcJ",
        "outputId": "cef2abe6-e2c0-42e8-ab2f-8e3c19dba1ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-e5e663c58c55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrdd\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reduce( ) – Reduces the records to single, we can use this to count or sum\n",
        "\n",
        "data = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
        "rdd  = sc.parallelize(data)\n",
        "\n",
        "num = rdd.reduce(lambda a,b: (a+b))\n",
        "print (f\"Max: {num}\")"
      ],
      "metadata": {
        "id": "U2G5cn1Domfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Aeg264po0f_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pair RDDs\n",
        "\n",
        "Spark Paired RDDs are RDDs containing a key-value pair. Key-value pair (KVP) consists of a two linked data item in it. Here, the key is the identifier, whereas value is the data corresponding to the key value."
      ],
      "metadata": {
        "id": "P5JQGB8go-6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating Pair RDDs\n",
        "Two common ways to create pair RDD:\n",
        "\n",
        "* From a list of key-value tuples\n",
        "* From a regular RDD"
      ],
      "metadata": {
        "id": "3hxK-CL4xovT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Pair RDD from regular RDD\n",
        "\n",
        "rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
        "sorted(rdd.map(lambda x: (x, 1)).collect())"
      ],
      "metadata": {
        "id": "eQVy8b_nxzzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Pair RDD from a list\n",
        "\n",
        "rdd = sc.parallelize([(1,\"a\"), (2,\"b\"), (3,\"c\")])\n",
        "rdd.collect()"
      ],
      "metadata": {
        "id": "tCGfYEfkx7d2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Transformations on pair RDDs\n",
        "\n",
        "All regular transformations work on pair RDD. Have to pass functions that operate on key value pairs rather than on individual elements"
      ],
      "metadata": {
        "id": "cBnHZy3byBZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reduceByKey(fun) - groups all the values with the same key\n",
        "\n",
        "rdd = sc.parallelize([(\"a\",1), (\"b\",2), (\"c\", 10),(\"a\", 2), (\"d\", 5), (\"a\", 4) ])\n",
        "rdd_reduceByKey = rdd.reduceByKey(lambda x, y: x+y )\n",
        "rdd_reduceByKey.collect()"
      ],
      "metadata": {
        "id": "wwpHth-TyMrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sortByKey(fun) - Order RDD pair by key\n",
        "\n",
        "rdd = sc.parallelize([(\"a\",1), (\"c\",2), (\"b\", 10),(\"a\", 2), (\"d\", 5), (\"a\", 4) ])\n",
        "rdd_reduceByKey = rdd.reduceByKey(lambda x, y: x+y )\n",
        "rdd_reduceByKey.sortByKey(ascending = True).collect()"
      ],
      "metadata": {
        "id": "NHwTbpWyyRyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# groupByKey( ) - Groups all the values with the same key in the pair\n",
        "\n",
        "rdd = sc.parallelize([(\"a\",1), (\"c\",2), (\"b\", 10),(\"a\", 2), (\"d\", 5), (\"a\", 4) ])\n",
        "rdd_groupByKey = rdd.groupByKey().collect()\n",
        "for letter, value in  rdd_groupByKey:\n",
        "    print (letter, list(value))"
      ],
      "metadata": {
        "id": "HcGYGzxLyVo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# join( ) - transformation joins the two pair RDDs based on their key\n",
        "\n",
        "rdd01 = sc.parallelize([(\"a\",1), (\"b\", 5),(\"c\", 7) ])\n",
        "rdd02 = sc.parallelize([(\"a\",2), (\"b\", 3),(\"d\", 4) ])\n",
        "\n",
        "rdd01. join(rdd02).collect()"
      ],
      "metadata": {
        "id": "Zzx2RbH9yV3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# countByKey( ) - action counts the number of elements for each key\n",
        "\n",
        "rdd = sc.parallelize([(\"a\",2), (\"b\", 4),(\"a\", 3) ])\n",
        "for key, val in  rdd.countByKey().items():\n",
        "    print (key, val)"
      ],
      "metadata": {
        "id": "3HiAIbCNyf7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# collectAsMap( ) - action return the key-value pairs in the RDD as a dictionary\n",
        "\n",
        "rdd = sc.parallelize([(\"a\",2), (\"b\", 4),(\"c\", 3) ])\n",
        "rdd.collectAsMap()"
      ],
      "metadata": {
        "id": "GdOxOJK3ygDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example compute-intensive tasks\n",
        "\n",
        "Spark can also be used for compute-intensive tasks. \n",
        "\n",
        "This code estimates π by \"throwing darts\" at a circle. We pick random points in the unit square ((0, 0) to (1,1)) and see how many fall in the unit circle. The fraction should be π / 4, so we use this to get our estimate."
      ],
      "metadata": {
        "id": "C1nAue2JPfVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inside(p):\n",
        "    x, y = random.random(), random.random()\n",
        "    return x*x + y*y < 1\n",
        "\n",
        "count = sc.parallelize(range(0, NUM_SAMPLES)) \\\n",
        "             .filter(inside).count()\n",
        "print(\"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES))"
      ],
      "metadata": {
        "id": "yjt7bkykP8uO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataFrame\n",
        "\n",
        "\n",
        "A DataFrame is a two-dimensional labeled data structure with columns of potentially different types. You can think of a DataFrame like a spreadsheet, a SQL table, or a dictionary of series objects. Apache Spark DataFrames provide a rich set of functions (select columns, filter, join, aggregate) that allow you to solve common data analysis problems efficiently.\n",
        "\n",
        "Apache Spark DataFrames are an abstraction built on top of Resilient Distributed Datasets (RDDs). Spark DataFrames and Spark SQL use a unified planning and optimization engine, allowing you to get nearly identical performance across all supported languages (Python, SQL, Scala, and R)"
      ],
      "metadata": {
        "id": "X4yWgGcS00XZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a Spark Session"
      ],
      "metadata": {
        "id": "i2eQzRys84BD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a Spark Session\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# create a spark session\n",
        "spark = SparkSession.builder.master(\"local[1]\").appName('SparkDataFrame').getOrCreate()\n",
        "\n",
        "spark \n",
        "\n"
      ],
      "metadata": {
        "id": "ul1ATYdZ8k-l",
        "outputId": "e3781cf3-6acb-4b1b-bb65-4f9fbb26af19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ff7c79bd9d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://8bd57dd1a04b:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Intro pyspark</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Create a DataFrame "
      ],
      "metadata": {
        "id": "VWWZKsXy_s7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Create a DataFrame from RDD\n",
        "\n",
        "Most Apache Spark queries return a DataFrame. This includes reading from a table, loading data from files, and operations that transform data."
      ],
      "metadata": {
        "id": "R3fePRvh7spP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iphones_RDD = sc.parallelize([ (\"XS\", 2018, 5.65, 2.79, 6.24), \\\n",
        "(\"XR\", 2018, 5.94, 2.98, 6.84),\\\n",
        "(\"X10\", 2017, 5.65, 2.79, 6.13),\\\n",
        "(\"8Plus\", 2017, 6.23, 3.07, 7.12)\\\n",
        "])\n",
        "\n",
        "names = ['Model', 'Year', 'Height', 'Width', 'Weight']\n",
        "\n",
        "iphones_df = spark.createDataFrame(iphones_RDD, schema=names)\n",
        "iphones_df.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "xXnIuBFH7qMA",
        "outputId": "d99c6fb4-58ef-4b5b-845c-1988d83cc365",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----+------+-----+------+\n",
            "|Model|Year|Height|Width|Weight|\n",
            "+-----+----+------+-----+------+\n",
            "|   XS|2018|  5.65| 2.79|  6.24|\n",
            "|   XR|2018|  5.94| 2.98|  6.84|\n",
            "|  X10|2017|  5.65| 2.79|  6.13|\n",
            "|8Plus|2017|  6.23| 3.07|  7.12|\n",
            "+-----+----+------+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Create a DataFrame from pandas\n",
        "\n",
        "ou can also create a Spark DataFrame from a list or a pandas DataFrame, such as in the following example\n"
      ],
      "metadata": {
        "id": "QdFZqA3C9F9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = [[1, \"Elia\"], [2, \"Teo\"], [3, \"Fang\"]]\n",
        "\n",
        "pdf = pd.DataFrame(data, columns=[\"i\", \"n\"])\n",
        "\n",
        "df1 = spark.createDataFrame(pdf)\n",
        "df1.show()\n",
        "\n",
        "df2 = spark.createDataFrame(data, schema=\"id LONG, name STRING\")\n",
        "df2.show()"
      ],
      "metadata": {
        "id": "H9NP4NBd9PpH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02ab6d2b-b715-4942-ae88-46fd45638816"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+\n",
            "|  i|   n|\n",
            "+---+----+\n",
            "|  1|Elia|\n",
            "|  2| Teo|\n",
            "|  3|Fang|\n",
            "+---+----+\n",
            "\n",
            "+---+----+\n",
            "| id|name|\n",
            "+---+----+\n",
            "|  1|Elia|\n",
            "|  2| Teo|\n",
            "|  3|Fang|\n",
            "+---+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load data into a DataFrame from files\n",
        "\n",
        "You can load data from many supported file formats. The following example uses a dataset available in the datasets directory."
      ],
      "metadata": {
        "id": "807QjYxu9Wt2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "df_csv = spark.read.csv(\"us-counties-2021.csv\", sep=',', header=True, inferSchema=True)\n",
        "\n",
        "df_csv.tail(3)"
      ],
      "metadata": {
        "id": "OzUiNGKx9iDr",
        "outputId": "f3ae4f20-7bcc-4116-b7b3-a1d454034a9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(date=datetime.datetime(2021, 12, 31, 0, 0), county='Uinta', state='Wyoming', fips=56041, cases=4154, deaths=31),\n",
              " Row(date=datetime.datetime(2021, 12, 31, 0, 0), county='Washakie', state='Wyoming', fips=56043, cases=1879, deaths=37),\n",
              " Row(date=datetime.datetime(2021, 12, 31, 0, 0), county='Weston', state='Wyoming', fips=56045, cases=1254, deaths=14)]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataFrame Operations\n",
        "\n",
        "Like RDD Spark suporte  support two types of Data Frame operations: transformations and actions\n",
        " \n",
        "* Transformations  create a new dataset from an existing one.\n",
        "* Actions, which return a value to the driver program after running a computation on the dataset. \n",
        "\n",
        "For example, map is a transformation that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, reduce is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program (although there is also a parallel reduceByKey that returns a distributed dataset).\n",
        "\n",
        "All transformations in Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. This design enables Spark to run more efficiently. For example, we can realize that a dataset created through map will be used in a reduce and return only the result of the reduce to the driver, rather than the larger mapped dataset.\n",
        "\n",
        "By default, each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory using the persist (or cache) method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it. There is also support for persisting RDDs on disk, or replicated across multiple nodes."
      ],
      "metadata": {
        "id": "eqUtx1FX_VsC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformations"
      ],
      "metadata": {
        "id": "m8ZP9BfjANHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select( ) - Transformation subsets the columns in the DataFrame\n",
        "\n",
        "\n",
        "\n",
        "covidDF = spark.read.csv(\"us-counties-2021.csv\", header=True, inferSchema=True)\n",
        "\n",
        "county_value_df = covidDF.select(\"county\")\n",
        "\n",
        "county_value_df.show(10)"
      ],
      "metadata": {
        "id": "kMSTmDynAUgg",
        "outputId": "3f3f2004-1196-45b9-9a6e-8e34e1ad806f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|  county|\n",
            "+--------+\n",
            "| Autauga|\n",
            "| Baldwin|\n",
            "| Barbour|\n",
            "|    Bibb|\n",
            "|  Blount|\n",
            "| Bullock|\n",
            "|  Butler|\n",
            "| Calhoun|\n",
            "|Chambers|\n",
            "|Cherokee|\n",
            "+--------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# filter( ) transformation filters out the rows based on a condition\n",
        "\n",
        "\n",
        "covidDF = spark.read.csv(\"us-counties-2021.csv\", header=True, inferSchema=True)\n",
        "\n",
        "new_df_covidDF = covidDF.filter(covidDF.county ==  \"Bibb\")\n",
        "\n",
        "new_df_covidDF.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzQmEoSaMJD7",
        "outputId": "d7ab477d-6a08-40f1-9ad8-a5e5b0bb7228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+------+-------+-----+-----+------+\n",
            "|               date|county|  state| fips|cases|deaths|\n",
            "+-------------------+------+-------+-----+-----+------+\n",
            "|2021-01-01 00:00:00|  Bibb|Alabama| 1007| 1854|    46|\n",
            "|2021-01-01 00:00:00|  Bibb|Georgia|13021|10273|   261|\n",
            "|2021-01-02 00:00:00|  Bibb|Alabama| 1007| 1863|    46|\n",
            "|2021-01-02 00:00:00|  Bibb|Georgia|13021|10433|   261|\n",
            "|2021-01-03 00:00:00|  Bibb|Alabama| 1007| 1882|    46|\n",
            "+-------------------+------+-------+-----+-----+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# groupby() can be used to group a variable\n",
        "\n",
        "covidDF = spark.read.csv(\"us-counties-2021.csv\", header=True, inferSchema=True)\n",
        "\n",
        "new_df_groupByDeaths = covidDF.groupby('deaths')\n",
        "new_df_groupByDeaths.count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxJAqEgVM-kr",
        "outputId": "77ee0a5e-43ed-42b4-938d-e23182c0f5fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "|deaths|count|\n",
            "+------+-----+\n",
            "|   463|  258|\n",
            "|   148| 1200|\n",
            "|   833|   95|\n",
            "|   496|  149|\n",
            "|  1342|   26|\n",
            "|  1088|   32|\n",
            "|  3794|    7|\n",
            "|  1645|   17|\n",
            "|  2866|    9|\n",
            "|   471|  180|\n",
            "|  1829|   16|\n",
            "|  1238|   22|\n",
            "|  3997|    1|\n",
            "|  2142|   10|\n",
            "|  1591|   16|\n",
            "|  3749|    6|\n",
            "|  1959|   14|\n",
            "|  1580|    7|\n",
            "|  2366|   11|\n",
            "|  3918|    1|\n",
            "+------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# orderBy() operation sorts the DataFrame based on one or more columns\n",
        "\n",
        "covidDF = spark.read.csv(\"us-counties-2021.csv\", header=True, inferSchema=True)\n",
        "\n",
        "new_df_groupByDate = covidDF.groupby('date')\n",
        "new_df_groupByDate.count().orderBy('date').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLBODkfuNX_-",
        "outputId": "4001f014-8ad5-426a-cc05-b20073b23275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----+\n",
            "|               date|count|\n",
            "+-------------------+-----+\n",
            "|2021-01-01 00:00:00| 3245|\n",
            "|2021-01-02 00:00:00| 3247|\n",
            "|2021-01-03 00:00:00| 3246|\n",
            "|2021-01-04 00:00:00| 3246|\n",
            "|2021-01-05 00:00:00| 3245|\n",
            "|2021-01-06 00:00:00| 3245|\n",
            "|2021-01-07 00:00:00| 3246|\n",
            "|2021-01-08 00:00:00| 3245|\n",
            "|2021-01-09 00:00:00| 3245|\n",
            "|2021-01-10 00:00:00| 3245|\n",
            "|2021-01-11 00:00:00| 3245|\n",
            "|2021-01-12 00:00:00| 3245|\n",
            "|2021-01-13 00:00:00| 3246|\n",
            "|2021-01-14 00:00:00| 3245|\n",
            "|2021-01-15 00:00:00| 3245|\n",
            "|2021-01-16 00:00:00| 3245|\n",
            "|2021-01-17 00:00:00| 3245|\n",
            "|2021-01-18 00:00:00| 3245|\n",
            "|2021-01-19 00:00:00| 3247|\n",
            "|2021-01-20 00:00:00| 3246|\n",
            "+-------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dropDuplicates() removes the duplicate rows of a DataFrame\n",
        "\n",
        "covidDF = spark.read.csv(\"us-counties-2021.csv\", header=True, inferSchema=True)\n",
        "\n",
        "newDf = covidDF.select('state','cases', 'deaths').dropDuplicates()\n",
        "newDf.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQqRutj-N1Jj",
        "outputId": "0a66da82-e9f7-4b88-c91f-0c758a6c9811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "715194"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# withColumnRenamed() renames a column in the DataFrame\n",
        "\n",
        "covidDF = spark.read.csv(\"us-counties-2021.csv\", header=True, inferSchema=True)\n",
        "\n",
        "newDf = covidDF.withColumnRenamed('deaths','mortes')\n",
        "newDf.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlN5HEhQOM9t",
        "outputId": "18d859ab-3ff9-493e-f47f-36e4f98076bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+---------+-------+----+-----+------+\n",
            "|               date|   county|  state|fips|cases|mortes|\n",
            "+-------------------+---------+-------+----+-----+------+\n",
            "|2021-01-01 00:00:00|  Autauga|Alabama|1001| 4239|    50|\n",
            "|2021-01-01 00:00:00|  Baldwin|Alabama|1003|13823|   169|\n",
            "|2021-01-01 00:00:00|  Barbour|Alabama|1005| 1517|    33|\n",
            "|2021-01-01 00:00:00|     Bibb|Alabama|1007| 1854|    46|\n",
            "|2021-01-01 00:00:00|   Blount|Alabama|1009| 4693|    63|\n",
            "|2021-01-01 00:00:00|  Bullock|Alabama|1011|  888|    22|\n",
            "|2021-01-01 00:00:00|   Butler|Alabama|1013| 1522|    45|\n",
            "|2021-01-01 00:00:00|  Calhoun|Alabama|1015| 9584|   157|\n",
            "|2021-01-01 00:00:00| Chambers|Alabama|1017| 2366|    63|\n",
            "|2021-01-01 00:00:00| Cherokee|Alabama|1019| 1429|    22|\n",
            "|2021-01-01 00:00:00|  Chilton|Alabama|1021| 3004|    54|\n",
            "|2021-01-01 00:00:00|  Choctaw|Alabama|1023|  491|    22|\n",
            "|2021-01-01 00:00:00|   Clarke|Alabama|1025| 2418|    26|\n",
            "|2021-01-01 00:00:00|     Clay|Alabama|1027| 1150|    34|\n",
            "|2021-01-01 00:00:00| Cleburne|Alabama|1029| 1036|    16|\n",
            "|2021-01-01 00:00:00|   Coffee|Alabama|1031| 3643|    38|\n",
            "|2021-01-01 00:00:00|  Colbert|Alabama|1033| 4701|    60|\n",
            "|2021-01-01 00:00:00|  Conecuh|Alabama|1035|  839|    17|\n",
            "|2021-01-01 00:00:00|    Coosa|Alabama|1037|  519|     7|\n",
            "|2021-01-01 00:00:00|Covington|Alabama|1039| 2931|    40|\n",
            "+-------------------+---------+-------+----+-----+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actions"
      ],
      "metadata": {
        "id": "OxP1aUXGOiT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# printSchema() method prints the types of columns in the DataFrame\n",
        "\n",
        "covidDF = spark.read.csv(\"us-counties-2021.csv\", header=True, inferSchema=True)\n",
        "covidDF.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_CalqO5OpQg",
        "outputId": "bb546d1e-5ae8-4d42-c360-75825ce3e7de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- date: timestamp (nullable = true)\n",
            " |-- county: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- fips: integer (nullable = true)\n",
            " |-- cases: integer (nullable = true)\n",
            " |-- deaths: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# columns prints the columns of a DataFrame\n",
        "\n",
        "covidDF = spark.read.csv(\"us-counties-2021.csv\", header=True, inferSchema=True)\n",
        "covidDF.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylBjbQq8O1U3",
        "outputId": "ae649c1c-3051-4b09-d332-c622bf63241c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['date', 'county', 'state', 'fips', 'cases', 'deaths']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# describe() operation compute summary statistics of numerical columns in the DataFrame\n",
        "\n",
        "covidDF = spark.read.csv(\"us-counties-2021.csv\", header=True, inferSchema=True)\n",
        "covidDF.describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxIsQZGrO9vs",
        "outputId": "aaf47f53-5147-498a-bd5d-f4837e96498e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+-------+------------------+------------------+------------------+\n",
            "|summary|   county|  state|              fips|             cases|            deaths|\n",
            "+-------+---------+-------+------------------+------------------+------------------+\n",
            "|  count|  1185373|1185373|           1174570|           1185373|           1156903|\n",
            "|   mean|     null|   null| 31471.97141081417| 11160.30975988149|193.60917553157006|\n",
            "| stddev|     null|   null|16365.330590115294|42189.206066249295| 881.2526727171902|\n",
            "|    min|Abbeville|Alabama|              1001|                 0|                 0|\n",
            "|    max|  Ziebach|Wyoming|             78030|           1697286|             35382|\n",
            "+-------+---------+-------+------------------+------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}